# Ralph Progress Log

## Codebase Patterns

(Add reusable patterns discovered during iterations here)

---

## Log

(Iteration entries will be appended below)

---

## 2026-02-08 - US-001

**Story:** Add --no-fix flag to review commands

### What was implemented
- Added `--no-fix` boolean flag (default: False) to the `loop()` command in `src/ralph/commands/loop.py`
- Added `--no-fix` boolean flag (default: False) to the `review()` command in `src/ralph/commands/review.py`
- Threaded `no_fix` parameter through `_run_review_loop()` in `loop.py` (signature + call site)
- Help text: "Report review findings without applying automated fixes"

### Tests written
- No new tests needed for this story (flag plumbing only; behavior implementation is US-002)
- All 217 existing tests pass

### Files changed
- `src/ralph/commands/loop.py` - added `no_fix` param to `loop()` and `_run_review_loop()`
- `src/ralph/commands/review.py` - added `no_fix` param to `review()`
- `plans/TASKS.json` - marked US-001 as passes: true

### Learnings for future iterations
- Typer `--no-fix` automatically maps to `no_fix` Python parameter via Typer's flag naming convention
- The `_run_review_loop()` function in loop.py is the bridge between the loop command and the review pipeline; subsequent stories (US-002, US-003) will wire no_fix into ReviewLoopService

---

[Ralph Loop] 2026-02-08 05:55 UTC - Iteration 1: US-001 (Add --no-fix flag to review commands) completed

## 2026-02-08 - US-002

**Story:** Implement --no-fix behavior in ReviewLoopService

### What was implemented
- Added `no_fix` keyword parameter (default: False) to `ReviewLoopService.run_review_loop()` method signature
- When `no_fix=True`, the `FixLoopService.run_fix_loop()` call is skipped after a NEEDS_WORK verdict
- Logs `[Fix] Skipped (--no-fix)` via `logger.info()` when a fix is skipped
- Continues to the next reviewer after logging (pipeline is not aborted)

### Tests written
- `test_no_fix_skips_fix_loop_on_needs_work` - verifies FixLoopService is not instantiated when no_fix=True
- `test_no_fix_logs_skip_message` - verifies '[Fix] Skipped (--no-fix)' appears in log records
- `test_no_fix_continues_to_next_reviewer` - verifies both reviewers in a 2-reviewer pipeline execute when no_fix=True
- `test_no_fix_false_still_runs_fix_loop` - verifies default behavior (no_fix=False) still invokes fix loop

### Files changed
- `src/ralph/services/review_loop.py` - added `no_fix` param to `run_review_loop()`, conditional skip of fix loop
- `tests/test_review_loop.py` - added 4 new tests, `logging` and `pytest` imports
- `plans/TASKS.json` - marked US-002 as passes: true

### Learnings for future iterations
- The commands (loop.py, review.py) currently iterate reviewers manually rather than calling `run_review_loop()`. Future stories (US-003) that add no_fix summary output to commands will need to handle this in both places.
- `caplog` fixture with `caplog.at_level(logging.INFO)` is the clean way to assert log messages in tests.

---

[Ralph Loop] 2026-02-08 05:58 UTC - Iteration 2: US-002 (Implement --no-fix behavior in ReviewLoopService) completed

## 2026-02-08 - US-003

**Story:** Add --no-fix summary to review output

### What was implemented
- Added `fix_skipped` field (default: False) to `ReviewerResult` NamedTuple in `review_loop.py`
- In `ReviewLoopService.run_review_loop()`, set `fix_skipped=True` on results when `no_fix=True` and reviewer has NEEDS_WORK findings eligible for fix loop
- Updated summary display in `_run_review_loop()` in `loop.py` to show `[FINDINGS] reviewer: findings (not fixed)` for fix-skipped reviewers
- Updated summary display in `review()` in `review.py` with the same logic
- Both summaries include `Findings (not fixed): N` count in the summary line when skipped_fix > 0
- Both commands detect `no_fix` + NEEDS_WORK + eligible fix loop and mark results with `fix_skipped=True`

### Tests written
- `TestNoFixSummary::test_no_fix_sets_fix_skipped_on_needs_work_result` - verifies fix_skipped=True on NEEDS_WORK results in run_review_loop
- `TestNoFixSummary::test_no_fix_does_not_set_fix_skipped_on_passed_result` - verifies fix_skipped=False on PASSED results
- `TestNoFixSummary::test_no_fix_false_does_not_set_fix_skipped` - verifies default behavior leaves fix_skipped=False
- `TestNoFixSummary::test_no_fix_mixed_results_only_marks_needs_work` - verifies only NEEDS_WORK reviewers get fix_skipped
- `TestReviewNoFixSummary::test_no_fix_shows_findings_not_fixed_in_summary` - verifies CLI output shows "findings (not fixed)"
- `TestReviewNoFixSummary::test_no_fix_includes_skipped_fix_count_in_summary_line` - verifies "Findings (not fixed): 1" in summary line
- `TestReviewNoFixSummary::test_no_fix_not_set_does_not_show_findings_not_fixed` - verifies no spurious output without --no-fix

### Files changed
- `src/ralph/services/review_loop.py` - added `fix_skipped` field to ReviewerResult, set it in run_review_loop()
- `src/ralph/commands/loop.py` - added Verdict import, no_fix-aware result marking, updated summary display
- `src/ralph/commands/review.py` - added Verdict import, no_fix-aware result marking, updated summary display
- `tests/test_review_loop.py` - added TestNoFixSummary class with 4 tests
- `tests/test_review_command.py` - added TestReviewNoFixSummary class with 3 tests, updated _make_mock_service()
- `plans/TASKS.json` - marked US-003 as passes: true

### Learnings for future iterations
- Both `loop.py::_run_review_loop()` and `review.py::review()` manually iterate reviewers rather than calling `ReviewLoopService.run_review_loop()`, so changes to summary display must be made in both places
- NamedTuple fields with defaults must come after fields without defaults; `fix_skipped: bool = False` works at the end
- When using MagicMock for ReviewerResult in integration tests, auto-generated attributes (like fix_skipped) return MagicMock objects which are truthy — switching to real ReviewerResult instances avoids this

---

[Ralph Loop] 2026-02-08 06:04 UTC - Iteration 3: US-003 (Add --no-fix summary to review output) completed

## 2026-02-08 - US-004

**Story:** Create ReviewState Pydantic model

### What was implemented
- Created `ReviewState` Pydantic model in `src/ralph/models/review_state.py` with fields: `reviewer_names`, `completed`, `timestamp`, `config_hash`
- Added `compute_config_hash()` static method that produces a deterministic SHA-256 hash from a list of `ReviewerConfig` objects (captures name, skill, level, sorted languages)
- Added `save()` instance method to write state to a JSON file
- Added `load()` class method to read state from a JSON file (returns `None` for missing/invalid files)
- Added `REVIEW_STATE_FILENAME` constant (`.ralph-review-state.json`)
- Exported `ReviewState` and `REVIEW_STATE_FILENAME` from `src/ralph/models/__init__.py`

### Tests written
- `TestReviewStateModel::test_create_review_state` - verifies model creation with all fields
- `TestReviewStateModel::test_completed_defaults_to_empty_dict` - verifies default factory
- `TestReviewStateModel::test_serialization_round_trip` - verifies JSON serialization/deserialization
- `TestComputeConfigHash::test_hash_is_deterministic` - same config produces same hash
- `TestComputeConfigHash::test_hash_is_hex_string` - valid 64-char hex digest
- `TestComputeConfigHash::test_hash_changes_with_different_config` - different configs differ
- `TestComputeConfigHash::test_hash_changes_when_level_changes` - level change detected
- `TestComputeConfigHash::test_hash_empty_list` - empty list produces valid hash
- `TestComputeConfigHash::test_hash_language_order_irrelevant` - language list order normalized
- `TestSaveAndLoad::test_save_creates_json_file` - file written with correct content
- `TestSaveAndLoad::test_load_valid_file` - valid file loaded correctly
- `TestSaveAndLoad::test_load_nonexistent_returns_none` - missing file returns None
- `TestSaveAndLoad::test_load_invalid_json_returns_none` - malformed JSON returns None
- `TestSaveAndLoad::test_load_invalid_schema_returns_none` - wrong schema returns None
- `TestSaveAndLoad::test_save_load_round_trip` - save and load are inverse operations
- `TestReviewStateFilename::test_filename_value` - constant has expected value

### Files changed
- `src/ralph/models/review_state.py` - new file, ReviewState model
- `src/ralph/models/__init__.py` - added ReviewState and REVIEW_STATE_FILENAME exports
- `tests/test_review_state.py` - new file, 16 tests
- `plans/TASKS.json` - marked US-004 as passes: true

### Learnings for future iterations
- The manifest model (`manifest.py`) uses `by_alias=True` for camelCase JSON, but ReviewState uses snake_case throughout since `.ralph-review-state.json` is internal-only
- `compute_config_hash` sorts languages to ensure order-independence; this is important since `ReviewerConfig.languages` is a plain list
- Follow the `load`/`save` pattern from `manifest.py` for consistency: `load` returns `None` on failure, `save` writes with trailing newline

---

[Ralph Loop] 2026-02-08 - Iteration 4: US-004 (Create ReviewState Pydantic model) completed

[Ralph Loop] 2026-02-08 06:07 UTC - Iteration 4: US-004 (Create ReviewState Pydantic model) completed

## 2026-02-08 - US-005

**Story:** Implement resumable review logic in review commands

### What was implemented
- Added `--resume-review` boolean flag (default: False) to the `loop()` command in `src/ralph/commands/loop.py`
- Added `--resume-review` boolean flag (default: False) to the `review()` command in `src/ralph/commands/review.py`
- Threaded `resume_review` parameter through `_run_review_loop()` in `loop.py`
- Both commands load `ReviewState` from `.ralph-review-state.json` when `--resume-review` is set
- When state exists and config hash matches, already-completed reviewers are skipped
- When no state file exists or config hash mismatches, the full pipeline runs from the beginning
- State file is written/updated after each reviewer completes (via `ReviewState.save()`)

### Tests written
- `TestReviewResumeReview::test_resume_review_skips_completed_reviewers` - verifies completed reviewers from state file are skipped
- `TestReviewResumeReview::test_resume_review_runs_full_pipeline_without_state_file` - verifies full pipeline runs when no state exists
- `TestReviewResumeReview::test_resume_review_saves_state_after_each_reviewer` - verifies state file is created and updated
- `TestReviewResumeReview::test_resume_review_config_hash_mismatch_runs_full_pipeline` - verifies stale state is discarded

### Files changed
- `src/ralph/commands/loop.py` - added `resume_review` param to `loop()` and `_run_review_loop()`, added resume/state-save logic
- `src/ralph/commands/review.py` - added `resume_review` param to `review()`, added resume/state-save logic
- `tests/test_review_command.py` - added `TestReviewResumeReview` class with 4 tests
- `plans/TASKS.json` - marked US-005 as passes: true

### Learnings for future iterations
- The resume logic is implemented directly in both `_run_review_loop()` (loop.py) and `review()` (review.py) since both have their own reviewer iteration loops
- State file uses the existing `ReviewState` model from US-004 with `compute_config_hash()` for validation
- Config hash mismatch causes state to be discarded silently (logged but not errored)

---

[Ralph Loop] 2026-02-08 06:11 UTC - Iteration 5: US-005 (Implement resumable review logic in review commands) completed

## 2026-02-08 - US-006

**Story:** Handle state file cleanup and config invalidation

### What was implemented
- Added state file cleanup (`.ralph-review-state.json` deletion) after review loop completes in both `review.py` and `loop.py`'s `_run_review_loop()`
- Cleanup runs regardless of whether `--resume-review` was used — any leftover state file is removed
- Added visible console message when state is discarded due to config hash mismatch: "Review state discarded: reviewer configuration has changed"
- The config mismatch message appears in both `review.py` and `_run_review_loop()` in `loop.py`

### Tests written
- `TestReviewStateCleanup::test_state_file_cleaned_up_on_successful_completion` - verifies state file is deleted after all reviewers pass with `--resume-review`
- `TestReviewStateCleanup::test_state_file_cleaned_up_without_resume_flag` - verifies cleanup happens even without `--resume-review` flag
- `TestReviewStateCleanup::test_no_error_when_no_state_file_to_clean` - verifies no error when no state file exists to clean
- `TestReviewConfigInvalidation::test_config_change_logs_discard_message` - verifies console output includes "reviewer configuration has changed"
- `TestReviewConfigInvalidation::test_config_change_runs_all_reviewers_from_scratch` - verifies all reviewers run when config hash mismatches
- Updated `TestReviewResumeReview::test_resume_review_saves_state_after_each_reviewer` to account for post-completion cleanup

### Files changed
- `src/ralph/commands/review.py` - added state file cleanup on completion, added console message on config hash mismatch
- `src/ralph/commands/loop.py` - added state file cleanup in `_run_review_loop()`, added console message on config hash mismatch
- `tests/test_review_command.py` - added `TestReviewStateCleanup` (3 tests) and `TestReviewConfigInvalidation` (2 tests), updated 1 existing test
- `plans/TASKS.json` - marked US-006 as passes: true

### Learnings for future iterations
- When adding cleanup logic that removes files created during a loop, existing tests that assert file existence after the loop need to be updated to track intermediate writes (e.g., spy on `save()` calls)
- The `patch.object(ReviewState, "save", tracking_save)` pattern is useful for verifying intermediate save calls when the file gets cleaned up at the end

---

[Ralph Loop] 2026-02-08 06:15 UTC - Iteration 6: US-006 (Handle state file cleanup and config invalidation) completed

## 2026-02-08 - US-007

**Story:** Add .ralph-review-state.json to init scaffold

### What was implemented
- Added `create_gitignore()` method to `ScaffoldService` that creates or appends to `.gitignore`
- The method adds `.ralph-review-state.json` entry (using `REVIEW_STATE_FILENAME` constant)
- If `.gitignore` already exists, the entry is appended only if not already present
- Handles edge cases: missing trailing newline, empty file, duplicate prevention
- Updated `scaffold_all()` to call `create_gitignore()` and include `"gitignore"` in the result dict
- Imported `REVIEW_STATE_FILENAME` from `ralph.models.review_state`

### Tests written
- `TestCreateGitignore::test_creates_gitignore_when_not_exists` - verifies .gitignore is created with the entry
- `TestCreateGitignore::test_appends_to_existing_gitignore` - verifies entry is appended when .gitignore exists
- `TestCreateGitignore::test_does_not_duplicate_entry` - verifies no duplicate when entry already present
- `TestCreateGitignore::test_appends_with_newline_when_missing_trailing_newline` - verifies newline is added before entry
- `TestCreateGitignore::test_handles_empty_gitignore` - verifies entry is added to empty .gitignore
- `TestCreateGitignore::test_returns_gitignore_path` - verifies return value
- `TestScaffoldAllGitignore::test_scaffold_all_creates_gitignore` - verifies scaffold_all includes gitignore
- `TestScaffoldAllGitignore::test_scaffold_all_preserves_existing_gitignore` - verifies existing entries are preserved

### Files changed
- `src/ralph/services/scaffold.py` - added `create_gitignore()` method, updated `scaffold_all()`
- `tests/test_scaffold.py` - new file with 8 tests
- `plans/TASKS.json` - marked US-007 as passes: true

### Learnings for future iterations
- The `REVIEW_STATE_FILENAME` constant from `ralph.models.review_state` is reused to keep the gitignore entry in sync with the actual filename
- When appending to files, check for trailing newline to avoid malformed entries
- Line-based comparison (`splitlines()`) is more reliable than substring search for gitignore deduplication

---

[Ralph Loop] 2026-02-08 - Iteration 7: US-007 (Add .ralph-review-state.json to init scaffold) completed

[Ralph Loop] 2026-02-08 06:18 UTC - Iteration 7: US-007 (Add .ralph-review-state.json to init scaffold) completed

## 2026-02-08 - US-008

**Story:** Add codebase context to ralph tasks prompt

### What was implemented
- Added `_iter_file_tree()` generator function that walks the project directory, excluding common non-source dirs (.git, .venv, __pycache__, node_modules, etc.) with configurable max depth
- Added `_gather_codebase_summary()` function that produces a budget-capped summary containing a file tree and key file contents (pyproject.toml, CLAUDE.md, README.md, etc.)
- Extended `_build_prompt_from_skill()` to call `_gather_codebase_summary(Path.cwd())` and include the result in the prompt
- Added "Instructions for Already-Implemented Detection" section to the prompt instructing Claude to mark stories with `passes: true` when evidence exists in the codebase
- All codebase scanning is pure filesystem I/O — zero additional Claude API calls

### Tests written
- `TestIterFileTree` (5 tests): files/dirs listing, .git/.venv exclusion, max_depth respect, empty dir, egg-info exclusion
- `TestGatherCodebaseSummary` (6 tests): file tree section, pyproject.toml content, README content, missing key files, large file truncation, return type
- `TestBuildPromptIncludesCodebaseContext` (5 tests): codebase summary presence, already-implemented instructions, spec content preserved, branch name, file tree from cwd
- `TestCodebaseContextIntegration` (1 test): end-to-end tasks command sends codebase context to Claude

### Files changed
- `src/ralph/commands/tasks.py` - added _EXCLUDED_DIRS, _KEY_FILES, _MAX_SUMMARY_CHARS, _MAX_TREE_DEPTH constants; added _iter_file_tree(), _gather_codebase_summary(); modified _build_prompt_from_skill()
- `tests/test_tasks_codebase_context.py` - new file with 17 tests
- `plans/TASKS.json` - marked US-008 as passes: true

### Learnings for future iterations
- Using `Iterator[str]` yield pattern for file tree keeps memory usage low for large directories
- Budget-capping the codebase summary prevents oversized prompts; 12K chars is a reasonable limit
- The `_iter_file_tree` function sorts entries with directories first (using `p.is_file()` as sort key) for a cleaner tree display

---

[Ralph Loop] 2026-02-08 - Iteration 8: US-008 (Add codebase context to ralph tasks prompt) completed
