{
  "project": "RalphCLI",
  "branchName": "ralph/v2.1.0-enhancements",
  "description": "Point release adding --no-fix review-only mode, resumable review runs, and heuristic detection of already-implemented stories",
  "userStories": [
    {
      "id": "US-001",
      "title": "Add --no-fix flag to review commands",
      "description": "As a developer running reviews for audit, I want a --no-fix flag so that I can see findings without automated changes being made to my codebase.",
      "acceptanceCriteria": [
        "Add --no-fix boolean flag (default: false) to the ralph loop command signature in src/ralph/commands/loop.py",
        "Add --no-fix boolean flag (default: false) to the ralph review command signature in src/ralph/commands/review.py",
        "Thread the no_fix parameter through _run_review_loop() in loop.py",
        "Flag has help text visible via ralph loop --help and ralph review --help",
        "Typecheck passes",
        "Lint passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Added --no-fix flag to loop() and review() commands, threaded no_fix through _run_review_loop()."
    },
    {
      "id": "US-002",
      "title": "Implement --no-fix behavior in ReviewLoopService",
      "description": "As a developer, I want the --no-fix flag to skip FixLoopService.run_fix_loop() calls so that review findings are reported without code modifications.",
      "acceptanceCriteria": [
        "Add no_fix parameter to ReviewLoopService.run_review_loop() method signature",
        "When no_fix is true, skip the FixLoopService.run_fix_loop() call after a NEEDS_WORK verdict",
        "Log '[Fix] Skipped (--no-fix)' when a fix is skipped due to the flag",
        "Continue to the next reviewer after logging (do not abort the pipeline)",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Added no_fix parameter to run_review_loop(). When True, logs '[Fix] Skipped (--no-fix)' and continues to next reviewer."
    },
    {
      "id": "US-003",
      "title": "Add --no-fix summary to review output",
      "description": "As a developer running reviews in audit mode, I want the final summary to indicate which reviewers had findings that were not auto-fixed so that I can see what needs manual attention.",
      "acceptanceCriteria": [
        "Final review summary in both ralph loop and ralph review indicates which reviewers had NEEDS_WORK findings that were not auto-fixed due to --no-fix",
        "Summary shows a distinct status like 'findings (not fixed)' for skipped-fix reviewers",
        "Skipped-fix count is included in the summary line alongside passed/failed/skipped counts",
        "Unit tests verify summary output includes skipped-fix information when --no-fix is set",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Added fix_skipped field to ReviewerResult. Summary shows '[FINDINGS] reviewer: findings (not fixed)' and 'Findings (not fixed): N' count in both loop.py and review.py."
    },
    {
      "id": "US-004",
      "title": "Create ReviewState Pydantic model",
      "description": "As a developer, I need a ReviewState model to persist review progress so that interrupted review runs can be resumed.",
      "acceptanceCriteria": [
        "Create ReviewState Pydantic model in src/ralph/models/ with fields: reviewer_names (list of configured reviewer names), completed (dict mapping reviewer name to pass/fail status), timestamp (ISO 8601 string), and config_hash (string for detecting config changes)",
        "Model includes a class method or utility to compute a config hash from a list of ReviewerConfig objects",
        "Model includes methods to load from and save to .ralph-review-state.json",
        "Model is exported from src/ralph/models/__init__.py",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Created ReviewState Pydantic model with compute_config_hash, save/load methods, and REVIEW_STATE_FILENAME constant. Exported from models/__init__.py."
    },
    {
      "id": "US-005",
      "title": "Implement resumable review logic in review commands",
      "description": "As a developer with interrupted sessions, I want a --resume-review flag so that I can resume a long review pipeline without re-running completed reviewers.",
      "acceptanceCriteria": [
        "Add --resume-review flag to ralph loop command (default: false)",
        "Add --resume-review flag to ralph review command (default: false)",
        "When --resume-review is set and a valid state file exists with matching config hash, skip already-completed reviewers",
        "When --resume-review is set and no state file exists, run the full pipeline from the beginning",
        "State file is written/updated after each reviewer completes",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Added --resume-review flag to loop and review commands. Resume logic loads ReviewState, skips completed reviewers with matching config hash, and saves state after each reviewer completes."
    },
    {
      "id": "US-006",
      "title": "Handle state file cleanup and config invalidation",
      "description": "As a developer, I want the review state to be cleaned up on success and invalidated when config changes so that stale state never causes incorrect behavior.",
      "acceptanceCriteria": [
        "Clean up .ralph-review-state.json when the review loop completes successfully (all reviewers run)",
        "If reviewer configuration has changed since the state file was written (config hash mismatch), discard state and restart the full pipeline",
        "Log a message when state is discarded due to config change",
        "Unit tests verify cleanup on success and invalidation on config change",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "State file cleaned up after successful review completion in both review.py and loop.py. Config hash mismatch logs visible console message and discards stale state."
    },
    {
      "id": "US-007",
      "title": "Add .ralph-review-state.json to init scaffold",
      "description": "As a developer, I want .ralph-review-state.json to be gitignored by default so that review state files are not accidentally committed.",
      "acceptanceCriteria": [
        "ScaffoldService in src/ralph/services/scaffold.py gains a create_gitignore() method that creates or appends to .gitignore",
        "The .gitignore scaffold includes .ralph-review-state.json entry",
        "scaffold_all() calls create_gitignore()",
        "If .gitignore already exists, append the entry only if not already present",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Added create_gitignore() to ScaffoldService. Creates or appends .ralph-review-state.json to .gitignore. scaffold_all() includes gitignore in results."
    },
    {
      "id": "US-008",
      "title": "Add codebase context to ralph tasks prompt",
      "description": "As a developer generating tasks, I want Claude to receive codebase context alongside the spec so that it can identify requirements that appear to already be implemented.",
      "acceptanceCriteria": [
        "Extend _build_prompt_from_skill() in src/ralph/commands/tasks.py to gather a codebase summary (file tree, key file contents, or similar heuristic snapshot)",
        "Include the codebase summary in the prompt sent to Claude alongside the spec content",
        "Prompt instructs Claude to identify requirements that appear already implemented and mark them with passes: true and a note",
        "Codebase context gathering does not add more than one additional Claude API call (use filesystem scanning, not Claude)",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Added _iter_file_tree() and _gather_codebase_summary() for filesystem scanning. Prompt includes codebase summary and instructions for already-implemented detection. Zero additional Claude API calls."
    },
    {
      "id": "US-009",
      "title": "Parse and log already-implemented story detection",
      "description": "As a developer, I want to see a summary of stories that were detected as already implemented so that I understand what was auto-marked as complete.",
      "acceptanceCriteria": [
        "After parsing the TASKS.json output from Claude, count stories where passes is true",
        "Log a summary to the console: '[Tasks] N stories detected as already implemented'",
        "For each already-implemented story, display the story ID and title",
        "Stories detected as already-implemented retain their notes explaining the detection",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Added _log_already_implemented() function to tasks.py. Logs '[Tasks] N stories detected as already implemented' with story IDs and titles. Notes are retained on already-implemented stories."
    },
    {
      "id": "US-010",
      "title": "Bump version to 2.1.0 and update CHANGELOG",
      "description": "As a maintainer, I want the version bumped to 2.1.0 with a changelog entry so that the release is properly documented.",
      "acceptanceCriteria": [
        "Update version to 2.1.0 in pyproject.toml",
        "Update version to 2.1.0 in src/ralph/__init__.py",
        "Add changelog entry to CHANGELOG.md under [Unreleased] or [2.1.0] section with entries for --no-fix flag, resumable reviews, and already-implemented detection",
        "Typecheck passes",
        "Tests pass"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Bumped version to 2.1.0 in pyproject.toml and __init__.py. Added changelog entry with --no-fix flag, resumable reviews, and already-implemented detection."
    },
    {
      "id": "US-011",
      "title": "Add unit tests for --no-fix and resumable review features",
      "description": "As a developer, I want comprehensive tests for the new review features so that regressions are caught early.",
      "acceptanceCriteria": [
        "Tests verify --no-fix flag skips FixLoopService calls in ReviewLoopService.run_review_loop()",
        "Tests verify --no-fix summary output includes skipped-fix reviewer names",
        "Tests verify ReviewState model serialization, deserialization, and config hash computation",
        "Tests verify resume logic skips completed reviewers and runs remaining ones",
        "Tests verify state file cleanup on successful completion",
        "Tests verify config change invalidates stale state"
      ],
      "priority": 11,
      "passes": true,
      "notes": "Added 30 comprehensive tests in tests/test_nofix_and_resume.py covering --no-fix skip behavior, summary output, ReviewState serialization/hash, resume logic, state cleanup, and config invalidation across both review.py and loop.py code paths."
    },
    {
      "id": "US-012",
      "title": "Add unit tests for already-implemented story detection",
      "description": "As a developer, I want tests for the heuristic detection feature so that the codebase context gathering and summary logging are verified.",
      "acceptanceCriteria": [
        "Tests verify codebase summary is included in the prompt when generating tasks",
        "Tests verify stories with passes: true from Claude output are counted and logged correctly",
        "Tests verify the console summary message format: '[Tasks] N stories detected as already implemented'",
        "Tests verify normal operation when no stories are detected as already implemented",
        "All tests pass"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Added 19 tests in tests/test_already_implemented.py across 4 test classes: TestCodebaseSummaryInPrompt (5), TestAlreadyImplementedCounting (6), TestSummaryMessageFormat (4), TestNoStoriesDetected (4). Covers prompt context, counting/logging, message format, and normal operation."
    }
  ]
}
